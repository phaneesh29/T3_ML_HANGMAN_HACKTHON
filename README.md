# ğŸ§  RL DQN Agent for Hangman

## ğŸ“œ Overview
This project implements a **Deep Q-Network (DQN)**â€“based Reinforcement Learning agent to play the classic **Hangman** word-guessing game.  
The notebook demonstrates how an RL agent can learn optimal letter-guessing strategies through interaction and reward feedback instead of hardcoded heuristics.

---

## âš™ï¸ Features
- **DQN implementation using PyTorch**
- **Custom Hangman environment**
- **Experience replay and epsilon-greedy exploration**
- **Support for adjustable game parameters (word length, max lives, etc.)**
- **Training and evaluation routines**
- **Corpus-based vocabulary loading**

---

## ğŸ§© Project Structure
```
ml-hackathon-hmm.ipynb   # Main Jupyter Notebook (core implementation)
corpus.txt                # Word dataset file (required)
```

---

## ğŸ§  How It Works
1. **Environment Setup** â€“ The agent interacts with a Hangman environment that provides state feedback after each letter guess.  
2. **State Representation** â€“ Each game state encodes known letters, guesses, and remaining lives.  
3. **Action Space** â€“ The 26 English alphabet letters.  
4. **Reward Signal** â€“ Positive reward for correct guesses, negative for incorrect, and terminal penalties for losing.  
5. **DQN Agent** â€“ A neural network approximates the Q-value function.  
6. **Training Loop** â€“ The agent improves by playing thousands of episodes, updating the network using replay memory.

---

## ğŸ§° Requirements
Install the following before running:
```bash
pip install torch numpy matplotlib tqdm
```

---

## ğŸš€ Usage
1. Clone or download the project.  
2. Ensure `corpus.txt` is in the same folder as the notebook.  
3. Open the notebook:
   ```bash
   jupyter notebook ml-hackathon-hmm.ipynb
   ```
4. Run all cells to train and test the DQN agent.

---

## ğŸ“ˆ Evaluation
The notebook includes an evaluation method that tests the trained model across multiple episodes and measures:
- Average reward per episode  
- Win rate  
- Guess accuracy  

---

## âš ï¸ Limitations
- Requires a sufficiently large training set to generalize.  
- DQN may overfit short or repetitive words.  
- Slow training if GPU (CUDA) isnâ€™t available.  
- Limited exploration efficiency due to static epsilon decay.

---

## ğŸ§‘â€ğŸ’» Authors
Developed for an **ML Hackathon** exploring RL-based approaches to symbolic reasoning and word games.
- PES1UG23AM360
- PES1UG23AM314
- PES1UG23AM353
- PES1UG23AM352